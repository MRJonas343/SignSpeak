# SignSpeak ğŸ¤Ÿ

Traductor de Lengua de SeÃ±as a Texto con IA en tiempo real

## ğŸ“‹ DescripciÃ³n

SignSpeak es una aplicaciÃ³n web MVP que utiliza visiÃ³n por computadora e inteligencia artificial para traducir lengua de seÃ±as a texto en tiempo real. La aplicaciÃ³n captura video desde la cÃ¡mara del dispositivo, detecta las manos usando MediaPipe, y reconoce gestos bÃ¡sicos que se traducen a texto.

## âœ¨ CaracterÃ­sticas

- **Captura de video en tiempo real** desde la cÃ¡mara web
- **DetecciÃ³n de manos** usando MediaPipe Hands
- **Reconocimiento de gestos bÃ¡sicos**:
  - A - PuÃ±o cerrado
  - B - Mano abierta (5 dedos)
  - D - Solo dedo Ã­ndice
  - V - Ãndice y medio (paz/victoria)
  - W - Ãndice, medio y anular
  - ğŸ‘ - Pulgar arriba
- **Interfaz web simple y responsive** con FastAPI
- **TraducciÃ³n en tiempo real** mostrada en pantalla
- **FunciÃ³n de reseteo** para limpiar el texto reconocido

## ğŸ› ï¸ TecnologÃ­as

- **Backend**: Python 3.8+, FastAPI, Uvicorn
- **IA/ML**: MediaPipe, OpenCV
- **Frontend**: HTML5, CSS3, JavaScript

## ğŸ“¦ InstalaciÃ³n

### Prerrequisitos

- Python 3.8 o superior
- CÃ¡mara web funcional
- pip (gestor de paquetes de Python)

### Pasos

1. Clonar el repositorio:
```bash
git clone https://github.com/MRJonas343/SignSpeak.git
cd SignSpeak
```

2. Crear y activar un entorno virtual (recomendado):
```bash
# En Windows
python -m venv venv
venv\Scripts\activate

# En Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

3. Instalar las dependencias:
```bash
pip install -r requirements.txt
```

## ğŸš€ Uso

1. Iniciar la aplicaciÃ³n:
```bash
python app.py
```

2. Abrir un navegador web y navegar a:
```
http://localhost:5000
```

3. Permitir el acceso a la cÃ¡mara cuando se solicite

4. Realizar seÃ±as frente a la cÃ¡mara y observar el texto reconocido en tiempo real

5. Usar el botÃ³n "Limpiar Texto" para resetear la traducciÃ³n

## ğŸ“± CÃ³mo Funciona

1. **Captura de Video**: La aplicaciÃ³n accede a la cÃ¡mara web y captura frames en tiempo real
2. **DetecciÃ³n de Manos**: MediaPipe Hands detecta y rastrea 21 puntos clave de la mano
3. **Reconocimiento de Gestos**: El algoritmo analiza la posiciÃ³n de los dedos para reconocer gestos
4. **EstabilizaciÃ³n**: Se usa un buffer para evitar detecciones falsas y estabilizar el reconocimiento
5. **VisualizaciÃ³n**: El texto reconocido se muestra en la interfaz web en tiempo real

## ğŸ¯ Alcance MVP

Este es un MVP (Minimum Viable Product) enfocado en:
- Reconocimiento bÃ¡sico de 5-6 gestos/letras
- TraducciÃ³n simple a texto
- Interfaz funcional y limpia
- Pipeline completo: CÃ¡mara â†’ IA â†’ Texto

## ğŸ”® Futuras Mejoras

- [ ] Ampliar el vocabulario de seÃ±as (alfabeto completo A-Z)
- [ ] Reconocimiento de palabras y frases completas
- [ ] TraducciÃ³n de texto a voz (Text-to-Speech)
- [ ] Historial de traducciones
- [ ] ExportaciÃ³n de texto a archivo
- [ ] Soporte multilenguaje
- [ ] OptimizaciÃ³n para dispositivos mÃ³viles
- [ ] Entrenamiento de modelo personalizado con mÃ¡s gestos
- [ ] Modo de prÃ¡ctica/aprendizaje

## ğŸ¤ Contribuir

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT.

## ğŸ‘¥ Autor

- MRJonas343

## ğŸ™ Agradecimientos

- MediaPipe por su excelente framework de detecciÃ³n de manos
- La comunidad de cÃ³digo abierto